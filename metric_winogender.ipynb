{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rasikabhalerao/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/rasikabhalerao/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/rasikabhalerao/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/rasikabhalerao/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/rasikabhalerao/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/rasikabhalerao/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/rasikabhalerao/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/rasikabhalerao/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/rasikabhalerao/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/rasikabhalerao/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/rasikabhalerao/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/rasikabhalerao/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_data = pd.read_csv('winogender-schemas/data/all_sentences.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reformat data into dataframe with one row per 'template'\n",
    "Here, a 'template' is the winogender template x {participant, 'someone'} so there are 240\n",
    "\"\"\"\n",
    "\n",
    "df_data = pd.DataFrame(columns=['sentid', 'male_sentence', 'female_sentence', 'neutral_sentence'])\n",
    "cur_row = {}\n",
    "for index, row in df_raw_data.iterrows():\n",
    "    \n",
    "    sentid = row['sentid']\n",
    "    sentence = row['sentence']\n",
    "    \n",
    "    if cur_row.get('sentid') == None:\n",
    "        cur_row['sentid'] = '.'.join(sentid.split('.')[:3])\n",
    "    else:\n",
    "        assert cur_row['sentid'] == '.'.join(sentid.split('.')[:3])\n",
    "    \n",
    "    bias_cat = sentid.split('.')[3]\n",
    "    cur_row[bias_cat + '_' + 'sentence'] = sentence\n",
    "    \n",
    "    if len(cur_row) == 4:\n",
    "        df_data = df_data.append(cur_row, ignore_index=True)\n",
    "        cur_row = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reformat data to extract template mask for each template (longest prefix and longest suffix)\n",
    "Other columns are only the word tokens that are different for male, female, and neutral\n",
    "\"\"\"\n",
    "\n",
    "df_templates = pd.DataFrame(columns=['sentid', 'template', 'male_mask', 'female_mask', 'neutral_mask'])\n",
    "for index, row in df_data.iterrows():\n",
    "    \n",
    "    m = row['male_sentence'].strip().split()\n",
    "    f = row['female_sentence'].strip().split()\n",
    "    n = row['neutral_sentence'].strip().split()\n",
    "    \n",
    "    template_prefix = []\n",
    "    for i in range(len(m)):\n",
    "        if m[i] == f[i]:\n",
    "            template_prefix = template_prefix + [m[i]]\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    assert len(template_prefix) != len(m)\n",
    "    \n",
    "    template_suffix = []\n",
    "    for i in range(len(m)):\n",
    "        if m[-i-1] == f[-i-1]:\n",
    "            template_suffix = [m[-i-1]] + template_suffix\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    male_mask = ' '.join(m[len(template_prefix):-len(template_suffix)])\n",
    "    female_mask = ' '.join(f[len(template_prefix):-len(template_suffix)])\n",
    "    neutral_mask = ' '.join(n[len(template_prefix):-len(template_suffix)])\n",
    "    \n",
    "    template_prefix = ' '.join(template_prefix)\n",
    "    template_suffix = ' '.join(template_suffix)\n",
    "    \n",
    "    df_templates = df_templates.append({'sentid': row['sentid'],\n",
    "                                        'template': template_prefix + ' [MASK] ' + template_suffix,\n",
    "                                        'male_mask': male_mask,\n",
    "                                        'female_mask': female_mask,\n",
    "                                        'neutral_mask': neutral_mask,\n",
    "                                        }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BERT stuff\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "mask_token = tokenizer.mask_token\n",
    "softmax = torch.nn.LogSoftmax(dim=0)\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "\n",
    "def probability(sentence, masked_position):\n",
    "    \"\"\"\n",
    "    Given sentence and masked_position of token that we want probability of\n",
    "    Return probability of that token\n",
    "    \"\"\"\n",
    "    \n",
    "    unmasked_word = sentence[masked_position] #grab word\n",
    "    sentence[masked_position] = mask_token #re-mask word in sentence\n",
    "    sentence = ' '.join(sentence)\n",
    "\n",
    "    token_ids = tokenizer.encode(sentence, return_tensors='pt')\n",
    "    output = model(token_ids)\n",
    "    last_hidden_state = output[0].squeeze(0)\n",
    "    mask_hidden_state = last_hidden_state[masked_position]\n",
    "    probs = softmax(mask_hidden_state)\n",
    "\n",
    "    word_id = vocab.get(unmasked_word, None)\n",
    "    if word_id:\n",
    "        return probs[word_id].item()\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentence_left_to_right(to_unmask, unmasked):\n",
    "    \"\"\"\n",
    "    Given part in common between sentences (to_unmask) and part that is different (unmasked),\n",
    "    unmask the common part word by word. Return sum of logprobabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    [l, r] = to_unmask.split('[MASK]')\n",
    "    l = l.strip().split()\n",
    "    r = r.strip().split()\n",
    "    unmasked = unmasked.strip().split()\n",
    "    \n",
    "    score = 0\n",
    "    for i in range(len(l)):\n",
    "        masked_sentence = l[:i+1] + [mask_token]*(len(l)-i-1) + unmasked + [mask_token]*len(r)\n",
    "        prob = probability(masked_sentence, i)\n",
    "        if prob:\n",
    "            score = score + prob\n",
    "    \n",
    "    for i in range(len(r)):\n",
    "        masked_sentence = l + unmasked + r[:i+1] + [mask_token]*(len(r)-i-1)\n",
    "        prob = probability(masked_sentence, len(l)+len(unmasked)+i)\n",
    "        if prob:\n",
    "            score = score + prob\n",
    "    \n",
    "    return score\n",
    "\n",
    "def score_sentence_right_to_left(to_unmask, unmasked):\n",
    "    \"\"\"\n",
    "    Given part in common between sentences (to_unmask) and part that is different (unmasked),\n",
    "    unmask the common part word by word. Return sum of logprobabilities. Right to left.\n",
    "    \"\"\"\n",
    "    \n",
    "    [l, r] = to_unmask.split('[MASK]')\n",
    "    l = l.strip().split()\n",
    "    r = r.strip().split()\n",
    "    unmasked = unmasked.strip().split()\n",
    "    \n",
    "    score = 0\n",
    "    for i in range(len(r)):\n",
    "        masked_sentence = [mask_token]*len(l) + unmasked + [mask_token]*(len(r)-i-1) + r[-i-1:]\n",
    "        prob = probability(masked_sentence, len(masked_sentence)-i-1)\n",
    "        if prob:\n",
    "            score = score + prob\n",
    "    \n",
    "    for i in range(len(l)):\n",
    "        masked_sentence[len(l)-i-1] = l[-i-1]\n",
    "        masked_sentence = [mask_token]*(len(l)-i-1) + l[-i-1:] + unmasked + r\n",
    "        prob = probability(masked_sentence, len(l)-i-1)\n",
    "        if prob:\n",
    "            score = score + prob\n",
    "    \n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Score each sentence. Each row in the dataframe has the sentid and scores for male, female, and neutral.\n",
    "\"\"\"\n",
    "\n",
    "df_scores = pd.DataFrame(columns=['sentid', 'male_score', 'female_score', 'neutral_score'])\n",
    "for index, row in df_templates.iterrows():\n",
    "    template = row['template']\n",
    "    df_scores = df_scores.append({'sentid': row['sentid'],\n",
    "                                  'male_score': score_sentence_left_to_right(template, row['male_mask']),\n",
    "                                  'female_score': score_sentence_left_to_right(template, row['female_mask']),\n",
    "                                  'neutral_score': score_sentence_left_to_right(template, row['neutral_mask'])\n",
    "                                 },ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.to_csv('winogender_logsoftmax.csv')\n",
    "df_scores.head(len(df_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores['m>f'] = df_scores.apply(lambda row: row['male_score'] > row['female_score'], axis=1)\n",
    "df_scores['m-f'] = df_scores.apply(lambda row: row['male_score'] - row['female_score'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
