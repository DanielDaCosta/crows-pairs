<h1><a href="https://nyu-mll.github.io/nlu-debiasing-data">FAQ</a></h1>

<h2>Thanks for doing our HITs! We've been loving the work we've gotten so far, and with your help, we think we'll be able to build some pretty exciting technologies to help computers better understand human language.</h2>

<h3>Can you give more explanation about each attribute?</h3>
Here are some explanations (adapted from <a href="https://www.eeoc.gov/laws/">here</a>)

<ul>
<li><b>Race or color:</b> 
		Stereotyping or discriminating against someone because of their race or personal characteristics associated with race, such as hair texture, skin color, or certain facial features.</li>
<li><b>Gender, gender identity, or gender expression:</b> 
		Stereotyping or discriminating against someone because of their gender, or gender identity (transgender, non-conforming identity, or perceived gender identity).</li>
<li><b>Sexual orientation:</b> 
		Stereotyping or discriminating against someone because of their sexual orientation (gay, lesbian, straight, bisexual, asexual).</li>
<li><b>Religion:</b> 
		Stereotyping or discriminating against someone because of the religious, ethical, or moral beliefs they hold, or appear to hold.</li>
<li><b>Age:</b> 
		Stereotyping or discriminating against someone because of their age (old or young)</li>
<li><b>Nationality or citizenship status:</b> 
		Stereotyping or discriminating against someone based on the country or part of the world they appear to come from. For example, discrimination against someone because they appear Middle Eastern.</b></li>
<li><b>Disability:</b> 
		Stereotyping or discriminating against someone because of their history of disability or because they are believed to have some physical or mental impairment.</li>
<li><b>Physical appearance:</b> 
		Stereotyping or discriminating against someone because of physical characteristics like weight or how they dress.</b></li>
<li><b>Socioeconomic status and source of income:</b> 
		Stereotyping or discriminating against someone because they come from, or appear to come from, a lower socioeconomic status. </b></li>
</ul>

<h3>Will you reject any of my work?</h3>
No. This task is quite subjective, and we're happy to see your intuitions for how to answer, even if they don't quite match up with ours. We won't reject work just because we disagree with your judgments or your writing style, so as long as you make a reasonable attempt to answer the prompt, you'll be fine.

<h3>Can I select multiple attributes for a single HIT?</h3>
Yes! Please select all that are appropriate and that came to mind while you were writing the sentences.

<h3>Not a question, but I feel bad about writing with bias!</h3>
We understand. Please don’t feel bad about writing with bias! We know these aren’t your beliefs, and all the data is fully anonymized. Please feel free to reach out to us if you have any more concerns.

<h3>Do you have any preferences for how we construct sentences?</h3>
You are free to construct sentences in any way you see fit. However, please avoid systematic and repetitive sentences. Your writing should be as varied as possible, so don't stick to a single recipe to write the sentences. This is why we' We can't use highly repetitive data.

<h3>Can I write the same sentence in more than one field, or in more than one HIT?</h3>
No. If you accidentally reuse a sentence a couple of times in the course of doing many HITs, that's fine, but try to make every sentence unique.

<h3>Is there any limit to how many of these HITs I can do?</h3>
Nope! If you find the task interesting enough to be worth your time, please do lots of these!

<h3>When do you approve HITs?</h3>
We are grad students buried in work, so we can't promise to look at the data directly any more often than once a day.

<h3>Who are you?</h3>
We are the Bowman Group, a subgroup of the ML2 group at New York University Center for Data Science. We are also affiliated with the NYU Departments of Computer Science and Linguistics.
